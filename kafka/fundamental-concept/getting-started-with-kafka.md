# 카프카 입문
## 메세지와 배치
카프카에서 데이터의 기본 단위는 메세지다. 이는 데이터베이스의 로우나 레코드와 비슷해보일 수 있지만 카프카 입장에서 메세지는 단순한 '바이트의 배열'이기 때문에 메세지 데이터에는 특정한 형식이나 의미가 없다. 메세지는 key라 불리는 메타데이터를 포함할 수도 있다. 키 역시 메세지와 마찬가지로 카프카 입장에서는 특별한 의미가 없는 바이트 배열일 뿐이다. 키는 메세지를 저장할 파티션을 결정하기 위해 사용된다. 

카프카는 효율성을 위치 매세지를 배치 단위로 저장한다. 배치는 그저 같은 토픽의 파티션에 쓰여지는 메세지들의 집합일 뿐이다. 메세지를 쓸 때마다 네트워크 상에서 신호가 오가는 일은 막대한 오버헤드를 발생시키기 때문에 메세지를 배치 단위로 모아서 씀으로써 오버헤드를 줄인다. 하지만 이것은 지연과 처리량 사이의 트레이드오프를 발생시킨다. 즉, 배치 크기가 커질 수록 시간당 처리되는 메세지 수는 늘어나지만, 각각의 메세지가 전달되는 데에 걸리는 시간은 늘어나는 것이다. 배치는 더 효율적으로 데이터 전송과 저장을 하기 위해 약간의 처리 능력을 들여 압축되는 경우가 많다. 

## 스키마
카프카 입장에서 메세지는 단순한 바이트 배열일 뿐이지만 내용을 이해하기 쉽도록 일정한 구조(스키마)를 부여하는 것이 권장된다. 각 애플리케이션의 필요에 따라 사용할 수 있는 여러 스키마 중에 가장 간단한 것으로는 JSON이나 XML이 있다. 하지만 이 것들은 타입 처리 방식이나 스키마 버전 간의 호환성 유지성이 좋지 않다. 그래서 많은 카프카 사용자들은 Apache Avro를 선호한다. 이 것은 조밀한 직렬화 형식을 제공하고 메세지 본체와 스키마를 분리하기 때문에 스키마가 변경되더라도 코드를 수정할 필요가 없다. 강력한 데이터 타이핑과 스키마 변경에 따른 상위 호환성, 하위 호환성도 제공한다.

카프카에서는 일관된 데이터 형식이 중요하다. 데이터 형식이 일관되어야 메세지 쓰기와 읽기 작업 간의 결합도가 낮아져서 분리가 가능해지기 때문이다. 만약 이 작업들이 결합되어 있다면 '메세지를 발행하는 애플리케이션'의 신버전을 배포해야 할 때 먼저 '메세지를 구독하는 애플리케이션'을 먼저 구버전과 신버전 모두를 지원할 수 있도록 업데이트 해야 한다. 카프카는 잘 정의된 스키마를 공유 저장소에 저장함으로써 '두 버전 형식을 동시에 지원하도록 하는 작업' 없이 애플리케이션을 신버전으로 갱신할 수 있다. 

## 토픽과 파티션
카프카에 저장되는 메세지는 **토픽** 단위로 분류된다. 토픽과 가장 비슷한 개념으로는 데이터베이스의 테이블과 파일시스템의 폴더가 있다. 토픽은 다시 여러 개의 **파티션**으로 나뉘어진다. 커밋 로그의 관점으로 되돌아가자면, 파티션은 하나의 로그에 해당한다. 파티션에 메세지가 쓰여질 때는 '추가만 가능(append-only)'한 형태로 쓰여지며, 읽을 때는 맨 앞부터 마지막까지의 순서로 읽힌다. 대개 토픽에 여러 개의 파티션이 있는 만큼 토픽 안의 메세지 전체에 대해순서는 보장되지 않으며, 쓰여진 메세지들은 각 파티션의 끝에 추가된다. 파티션은 카프카가 데이터 중복과 확장성을 제공하는 방법이기도 하다. 각 파티션이 서로 다른 서버에 저장될 수 있기 때문에 하나의 토픽이 여러 개의 서버로 수평적으로 확장되어 하나의 서버가 갖는 걸 뛰어넘는 성능을 보여줄 수 있다. 그리고 서로 다른 서버들이 동일한 파티션의 복제본을 저장하고 있기 때문에 서버 중 하나에 장애가 발생한다고 해서 읽거나 쓸 수 없는 상황이 벌어지지 않는다.

카프카와 같은 시스템을 이야기할 땐 '**스트림**'이라는 용어가 자주 사용된다. 대부분의 경우 스트림은 파티션 개수에 상관없이 하나의 토픽에 저장된 데이터를 말하며, 프로듀서로부터 컨슈머로의 흐름, 하나의 데이터 흐름을 말한다. 메세지의 집합을 스트림이라 부르는 것은 카프카 스트림즈, 아파치 삼자, 아파치 스톰과 같은 프레임워크에서 메세지를 실시간으로 처리하는 것처럼 스트림 처리에 대한 논의를 진행할 때 가장 일반적인 일이다. 이런 방식의 처리는 데이터를 시간이 흐른 뒤 한꺼번에 대량으로 처리하는 하둡과 같은 오프라인 프레임워크와 대비된다. 


# 카프카의 기원
카프카는 링크드인 내부에서의 데이터 파이프라인 문제를 해결하기 위해 개발되었다. 카프카는 다양한 종류의 데이터를 다루고 고성능 메세지 교환 시스템 역할을 할 수 있도록 설계되었다.

## 링크드인이 직면한 문제
링크드인은 내부적으로 데이터를 저장하고 보여주는 커스텀 수집기와 오픈소스 툴에서 사용할 수 있도록 시스템의 지표와 애플리케이션의 지표를 수집하는 시스템을 돌리고 있었다. 이 모니터링 시스템이 보여주는 지표 중에는 CPU 사용률과 애플리케이션 성능과 같은 지표 외에 '복잡한 요청 추적 기능'이 있었는데, 이것은 하나의 사용자 요청이 내부의 여러 애플리케이션에 어떻게 전파되는지를 보여주는 것이었다. 하지만 이 모니터링 시스템엔 많은 결함이 있었다. 그런 결함 중 하나가 뭐였는지 살펴보면, 폴링 방식으로 수집되는 지표들이 있었는데 그 지표가 수집되는 간격이 긴 데다가 애플리케이션 담당자가 자신이 담당하는 애플리케이션에서 수집된 지표를 관리할 수 없게 되어 있었다. 이 모니터링 시스템은 손이 많이 가는 것이어서, 간단한 작업조차 사람이 일일이 직접 만져줘야 하는 데다가 똑같은 측정값도 시스템이 다르면 지표 이름까지 달라져서 일관성이 없었다. 

같은 시기에 '사용자 활동 정보'를 추적하기 위한 시스템도 있었다. 이것은 HTTP 서비스였는데, 프론트엔드 서버들이 이 시스템에 주기적으로 접속해서 XML 형식으로 되어 있는 메세지를 배치 방식으로 쏟아넣는 식으로 운영되었다. 이 메세지 배치들은 이후 오프라인 처리 플랫폼으로 옮겨져서 파싱되고 분석되었다. 이 시스템은 문제가 많았다. XML 형식은 일관성이 없고 파싱하는데에 많은 컴퓨팅 자원이 들어갔다. 추적되는 사용자 활동 유형을 변경하려면 프론트엔드와 오프라인 처리 시스템 간에 많은 추가 작업이 필요했다. 그러면서도 스키마 변경때문에 시스템이 계속 중단되고는 했다. 추적 작업 자체가 시간 단위로 처리되었기 때문에 실시간으로는 활용할 수 없었다.

모니터링과 사용자 활동 추적은 같은 백엔드 서비스를 사용할 수 없었다. 모니터링 서비스는 너무 단순하고, 데이터 형식은 활동 추적에 쓸 수 없었으며, 폴링 방식으로 작동했기 때문에 활동 추적에 사용되는 푸시 모델과 호환되지 않았다. 그리고 활동 추적 서비스는 모니터링 시스템에 쓰기엔 너무 자주 중단되었고, 배치 처리 방식은 실시간 모니터링과 경보 기능에 적합하지 않았다. 하지만, 모니터링 데이터와 활동 추적 데이터는 많은 속성들을 공유하는 데다가, 두 데이터의 상관관계는 매우 매력적이었다. 그런 매력적인 상관관계의 예로 "특정 유형의 사용자 활동이 애플리케이션 성능에 어떤 영향을 미치는지" 가 있다. 하지만 사용자 활동 정보를 배치 단위로 처리하는 데에 몇 시간씩 걸렸기 때문에 이런 유형의 문제에 대해 신속하게 대응할 수가 없었다.

처음 개발팀은 데이터에 실시간으로 접근하는 걸 가능케 하면서, 필요한 만큼의 메세지 트래픽을 처리할 수 있도록 수평 확장이 가능한 새로운 시스템을 찾기 위해 이미 나와 있는 오픈 소스 솔루션들을 면밀히 조사했다. 그러한 조사 과정에서 ActiveMQ를 사용해서 프로토타입 시스템을 개발해봤으나, 규모 확장성이 떨어졌다. 그리고 ActiveMQ에는 브로커를 정지시킬 수 있는 결함들이 많았다. 그런 결함은 클라이언트에 대한 연결에 영향을 줘서 사용자 요청을 서비스하는 애플리케이션 기능에 영향을 줄 수도 있었다. 

그래서 데이터 파이프라인 구축을 위한 커스텀 인프라스트럭처를 자체 개발하게 되었다.


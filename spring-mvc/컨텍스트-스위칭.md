# Spring WebMVC의 Thread-per-Request 모델
Spring WebMVC는 전통적인 Thread-per-Request 모델을 사용한다.

## 1. 기본 동작 구조
Spring WebMVC에서는 각 HTTP 요청이 들어오면:
1. 서블릿 컨테이너(Tomcat, Jetty 등)의 스레드 풀에서 하나의 스레드를 할당받는다 
2. 해당 스레드가 요청 처리의 전체 생명주기를 담당한다 
3. 컨트롤러 메서드 실행, 서비스 로직 처리, 데이터베이스 접근 등이 모두 같은 스레드에서 동기적으로 처리된다 
4. 응답이 완료될 때까지 해당 스레드는 다른 요청을 처리할 수 없다

## 2. 컨텍스트 스위칭 관점
이 모델의 이론상으로는 응답을 하기 전까지 기본적으로 컨텍스트 스위칭이 없다. 하나의 스레드가 요청을 끝까지 처리하는 구조이기 때문이다.

단, 몇 가지 예외 상황이 있다:
- **I/O 블로킹**: 데이터베이스 쿼리, 외부 API 호출 등에서 스레드가 대기할 때
- **외부 API 요청**
- **동기화 대기**: synchronized 블록이나 Lock에서 대기할 때 
- **CPU 코어 수를 넘기는 요청 수**: CPU 코어 수보다 많은 스레드 수를 가질 때

## 3. 장단점
**장점:**
- 구현이 단순하고 디버깅이 쉬움 
- 스레드 로컬 변수 사용 가능 
- 전통적인 동기 프로그래밍 모델

**단점:**
- 동시 처리 가능한 요청 수가 스레드 풀 크기에 제한됨 
- I/O 대기 시간 동안 스레드 자원 낭비 
- 높은 동시성 환경에서 성능 한계
이런 한계를 극복하기 위해 Spring WebFlux 같은 리액티브 프로그래밍 모델이 등장했다.

# 왜 WebMVC는 컨텍스트 스위칭 오버헤드가 높을까?

## 1. 스레드 수 자체가 많음  
WebMVC는 동시 요청 처리를 위해 많은 스레드를 생성해야 한다:

```
- 1000개 동시 요청 처리 = 1000개 스레드 필요
- WebFlux의 경우 = CPU 코어 수만큼의 이벤트 루프 스레드 (보통 8개 내외)
```

스레드가 많을수록 운영체제 스케줄러가 더 자주 컨텍스트 스위칭을 수행해야 한다.

## 2. I/O 블로킹으로 인한 잦은 대기  
WebMVC에서 가장 큰 문제는 I/O 대기 시간입니다:

```java
@GetMapping("/users/{id}")
public User getUser(@PathVariable Long id) {
    User user = userService.findById(id);  // DB 쿼리 - 스레드 블로킹
    Profile profile = profileService.getProfile(id);  // 외부 API 호출 - 스레드 블로킹
    return user;
}
```

각 I/O 작업에서 스레드가 블로킹되면:
- 운영체제가 해당 스레드를 대기 상태로 전환 
- 다른 실행 가능한 스레드로 컨텍스트 스위칭 
- I/O 완료 시 다시 원래 스레드로 컨텍스트 스위칭

## 3. 스레드 풀 경합
많은 스레드들이 제한된 자원을 두고 경합하면서:
- **스레드 간 경합**: 데이터베이스 커넥션 풀, 캐시 등 
- **동기화 오버헤드**: synchronized 블록, Lock 사용 
- **메모리 압박**: 각 스레드마다 스택 메모리 할당 (보통 1MB씩)

## 4. WebFlux와의 비교
WebFlux는 **이벤트 루프** 모델을 사용:

```java
@GetMapping("/users/{id}")
public Mono<User> getUser(@PathVariable Long id) {
    return userService.findById(id)  // 논블로킹
        .flatMap(user -> profileService.getProfile(id)  // 논블로킹
            .map(profile -> user));
}
```

- 적은 수의 스레드로 많은 요청 처리 
- I/O 대기 시 스레드가 다른 작업 처리 
- 전체적으로 컨텍스트 스위칭 빈도 감소

## 결론
WebMVC의 컨텍스트 스위칭 오버헤드가 높다는 것은 개별 요청 처리 중에 컨텍스트 스위칭이 많다는 의미가 아니라 시스템 전체적으로 많은 스레드와 잦은 I/O 블로킹으로 인해 컨텍스트 스위칭이 빈번하게 발생한다는 의미이다.   
높은 동시성과 I/O 집약적인 환경에서 이런 오버헤드가 더욱 두드러지게 나타납니다.


# 구체적으로 어떻게 컨텍스트 스위칭을 수행할까?

## 1. OS 스케줄링과 컨텍스트 스위칭
CPU 코어 수보다 많은 스레드가 있으면 OS 스케줄러가 지속적으로 컨텍스트 스위칭을 수행한다.

```
예시: 4코어 CPU + 1000개 스레드
- 동시에 실행 가능한 스레드: 4개
- 나머지 996개 스레드: 대기 상태 (RUNNABLE, BLOCKED, WAITING)
- OS가 시분할(time-slicing)로 스레드들을 번갈아 실행
```  
  
## 2. Thread-per-Request 모델의 실제 동작
"**응답 전까지 컨텍스트 스위칭이 일어나지 않는다**"는 모델의 이론일 뿐 보장되기 어렵다.

실제로는:
### 2.1 스레드 상태 변화가 빈번함
```java
@GetMapping("/user")
public User getUser() {
    // 1. RUNNABLE 상태에서 실행 중
    User user = userRepository.findById(1L);  
    // 2. DB I/O 대기 → BLOCKED 상태로 전환 (컨텍스트 스위칭 발생)
    // 3. DB 응답 도착 → RUNNABLE 상태로 복귀 (컨텍스트 스위칭 발생)
    
    String data = externalApiCall();  
    // 4. HTTP 호출 대기 → BLOCKED 상태 (컨텍스트 스위칭 발생)
    // 5. 응답 도착 → RUNNABLE 상태 (컨텍스트 스위칭 발생)
    
    return user;
}
```

### 2.2 실제 CPU 점유 패턴
```
스레드 A: [실행 2ms] → [I/O 대기 100ms] → [실행 1ms] → [I/O 대기 50ms] → [실행 1ms]
스레드 B: ────────────── [실행 3ms] → [I/O 대기 80ms] → [실행 2ms] ──────────
스레드 C: ───────────────────────── [실행 1ms] → [I/O 대기 120ms] ───────────
```

각 스레드는 **논리적으로는** 하나의 요청을 처리하지만, **물리적으로는** 수많은 컨텍스트 스위칭이 발생한다.

### 2.3 진짜 문제점들
1. **과도한 스레드 생성**
    ```
    - Tomcat 기본 max-threads: 200개
    - 높은 부하시: 수백~수천개 스레드 생성
    - 4코어 CPU로 200개 스레드를 돌리면 → 엄청난 컨텍스트 스위칭 오버헤드
    ```
2. **I/O 대기 시간의 비효율성**
    ```
    웹 애플리케이션의 전형적인 시간 분포:
    - 실제 CPU 작업: 5%
    - I/O 대기 시간: 95% (DB, 네트워크, 파일 시스템)
    ```
   스레드가 95%의 시간을 대기하면서 메모리만 차지하고 있다.
3. **메모리 압박**
    ```
    1000개 스레드 × 1MB 스택 = 1GB 메모리 소비
    실제 동시 작업: 4개 (CPU 코어 수)
    → 메모리 사용 효율성 극악
    ```
   
### 2.4 WebFlux와의 차이점
WebFlux는 이런 문제를 해결한다:

```java
// WebMVC: 스레드가 I/O 대기 중 블로킹
public User getUser() {
    return userRepository.findById(1L);  // 스레드 대기
}

// WebFlux: 스레드가 다른 작업 처리 가능
public Mono<User> getUser() {
    return userRepository.findById(1L)  // 논블로킹, 스레드 해제
        .subscribeOn(Schedulers.boundedElastic());
}
```

## 결론
Thread-per-Request 모델에서:
- **논리적으로는** 하나의 스레드가 하나의 요청을 처리 
- **물리적으로는** 수많은 컨텍스트 스위칭과 I/O 대기 발생 
- **결과적으로** 시스템 전체의 컨텍스트 스위칭 오버헤드가 매우 높음

따라서 "컨텍스트 스위칭 오버헤드가 높다"는 것은 개별 요청 처리의 연속성이 아니라, **시스템 전체의 비효율성**을 의미한다.


# 스레드는 하나지만 CPU 코어가 스위칭을 수행하는걸까?

## 1. 스레드 vs CPU 코어의 관계
```
하나의 CPU 코어 관점에서:
Time 0-10ms: Thread A 실행
Time 10-20ms: Thread B 실행  
Time 20-30ms: Thread C 실행
Time 30-40ms: Thread A 실행 (이어서)
Time 40-50ms: Thread D 실행
...
```

하나의 물리적 CPU 코어가 여러 스레드의 작업을 시분할로 처리한다.

## 2. 구체적인 예시
```java
// Thread-1: 사용자 A의 요청 처리
@GetMapping("/user/1")
public User getUser1() {
    // CPU 코어#1이 Thread-1 실행 (1ms)
    User user = userService.findById(1L);  
    // DB I/O 대기 → Thread-1이 BLOCKED 상태
    // CPU 코어#1이 Thread-2로 컨텍스트 스위칭!
    return user;
}

// Thread-2: 사용자 B의 요청 처리  
@GetMapping("/user/2")
public User getUser2() {
    // CPU 코어#1이 Thread-2 실행 (2ms)
    User user = userService.findById(2L);
    // DB I/O 대기 → Thread-2가 BLOCKED 상태
    // CPU 코어#1이 Thread-3으로 컨텍스트 스위칭!
    return user;
}
```

## 3. 실제 CPU 코어의 일생
**4코어 CPU + 200개 스레드**인 경우:
```
CPU Core #1의 시간대별 작업:
0-5ms:    Thread-1 (User A 요청)
5-10ms:   Thread-15 (User B 요청)
10-15ms:  Thread-3 (User C 요청)
15-20ms:  Thread-1 (User A 요청 계속) - I/O 완료로 복귀
20-25ms:  Thread-27 (User D 요청)
25-30ms:  Thread-15 (User B 요청 계속) - I/O 완료로 복귀
...
```

각 CPU 코어는 끊임없이 컨텍스트 스위칭하면서 수십~수백 개의 서로 다른 스레드 작업을 처리한다.

## 4. 스레드 입장에서 보면
```
Thread-1 (User A 요청)의 관점:
- 논리적으로: "나는 하나의 요청을 처리한다"
- 물리적으로: CPU 코어#1 → 대기 → CPU 코어#2 → 대기 → CPU 코어#1 → 완료
```

**하나의 스레드**가 **여러 CPU 코어에서 실행**될 수도 있고, **같은 CPU 코어**에서 **여러 번 실행**될 수 있다.

## 5. 왜 이게 비효율적인가?
1. **컨텍스트 스위칭 오버헤드**
    ```
    실제 작업 시간: 5ms
    컨텍스트 스위칭 시간: 0.1ms × 50회 = 5ms
    → 총 시간의 50%가 오버헤드!
    ```
2. **캐시 미스**
    ```
    Thread-1이 CPU 코어#1에서 실행 → L1/L2 캐시에 데이터 로드
    Thread-2가 CPU 코어#1에서 실행 → 캐시 무효화
    Thread-1이 다시 CPU 코어#1에서 실행 → 캐시 미스, 메모리에서 다시 로드
    ```

## 6. WebFlux의 차이점
    ```
    WebFlux (4코어 = 4개 이벤트 루프):
    CPU Core #1: Event Loop Thread #1 (계속 실행, 컨텍스트 스위칭 최소)
    CPU Core #2: Event Loop Thread #2 (계속 실행, 컨텍스트 스위칭 최소)
    CPU Core #3: Event Loop Thread #3 (계속 실행, 컨텍스트 스위칭 최소)
    CPU Core #4: Event Loop Thread #4 (계속 실행, 컨텍스트 스위칭 최소)
    ```
   
각 CPU 코어가 **하나의 이벤트 루프 스레드**를 계속 실행하면서 **수천 개의 요청**을 논블로킹으로 처리한다.

## 결론
- 스레드는 하나의 논리적 실행 단위 
- CPU 코어는 물리적 실행 자원 
- 하나의 CPU 코어가 여러 스레드를 번갈아 실행 
- 이 과정에서 지속적인 컨텍스트 스위칭 발생 
- 이것이 바로 Thread-per-Request 모델의 핵심 비효율성이다!

# 이벤트 루프의 컨텍스트 스위칭과의 차이
## 1. Thread-per-Request: OS 레벨 컨텍스트 스위칭
```java
// Thread-1 (User A)
public User getUser() {
    User user = userRepository.findById(1L);  // DB I/O 대기
    // → OS가 Thread-1을 BLOCKED 상태로 전환
    // → CPU 레지스터, 스택 포인터, 메모리 맵 등 전체 컨텍스트 저장
    // → 다른 스레드(Thread-2)로 완전한 컨텍스트 스위칭
    return user;
}
```

## 2. 이벤트 루프: 애플리케이션 레벨 컨텍스트 스위칭
```javascript
// 하나의 이벤트 루프 스레드에서
async function handleRequests() {
    // User A 요청 처리
    const userA = await userRepository.findById(1);  // 논블로킹
    // → 단순히 다음 이벤트로 넘어감 (같은 스레드 내에서)
    
    // User B 요청 처리  
    const userB = await userRepository.findById(2);  // 논블로킹
    // → 단순히 다음 이벤트로 넘어감 (같은 스레드 내에서)
}
```

## 3. 핵심 차이점들
### 3.1 컨텍스트 스위칭의 "무게"
**OS 레벨 스레드 컨텍스트 스위칭:**
```
저장/복원해야 할 것들:
- CPU 레지스터 (16개+)
- 스택 포인터
- 프로그램 카운터
- 메모리 맵 테이블
- 파일 디스크립터 테이블
- 시그널 마스크
- 페이지 테이블 전환
→ 비용: 약 1-10 마이크로초
```

**이벤트 루프 "컨텍스트 스위칭":**
```
저장/복원해야 할 것들:
- 함수 호출 스택의 실행 위치
- 로컬 변수들
→ 비용: 약 10-100 나노초 (100배 차이!)
```

### 3.2 메모리 사용량
**Thread-per-Request:**
```
1000개 동시 요청 = 1000개 스레드
각 스레드: 1MB 스택 메모리
총 메모리: 1GB
```

**이벤트 루프:**
```
1000개 동시 요청 = 1개 스레드 + 1000개 콜백 객체
각 콜백: 수십 바이트
총 메모리: 수십 KB
```

### 3.3 실제 실행 패턴
**Thread-per-Request 시나리오:**
```
Time 0-1ms:   Thread-1 실행 (User A)
Time 1-100ms: Thread-1 BLOCKED (DB 대기) → OS가 Thread-2로 스위칭
Time 100ms:   Thread-1 RUNNABLE → OS가 다시 Thread-1으로 스위칭
```

**이벤트 루프 시나리오:**
```
Time 0-1ms:   User A 요청 처리 시작 → DB 요청 발송 → 콜백 등록
Time 1-2ms:   User B 요청 처리 시작 → DB 요청 발송 → 콜백 등록  
Time 2-3ms:   User C 요청 처리 시작 → DB 요청 발송 → 콜백 등록
...
Time 100ms:   User A DB 응답 도착 → 콜백 실행 → 응답 완료
```

### 3.4 실제 벤치마크 예시
**Node.js(이벤트 루프)**
```javascript
// 10,000개 동시 요청 처리
const server = http.createServer(async (req, res) => {
    const data = await db.query('SELECT * FROM users WHERE id = ?', [req.params.id]);
    res.json(data);
});
// 결과: 8GB 메모리, 1개 스레드로 처리 가능
```

**Spring WebMVC (Thread-per-Request)**
```java
@GetMapping("/users/{id}")
public User getUser(@PathVariable Long id) {
    return userService.findById(id);
}
// 결과: 10,000개 스레드 × 1MB = 10GB 메모리 필요
```

## 결론
1. **OS 레벨 → 애플리케이션 레벨**로 컨텍스트 스위칭 이동 
2. **무거운 스레드 → 가벼운 콜백**으로 동시성 구현 
3. **메모리 사용량 100배 이상 감소** 
4. **CPU 캐시 지역성 향상** (같은 스레드에서 계속 실행)

이벤트 루프도 "컨텍스트 스위칭"을 하지만, 그 "무게" 가 완전히 다르다.